<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>SharedWorldModels</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Dreamer</h1>
        <p>Shared World Models</p>
        <p class="view"><a href="http://github.com/deepdad/SharedWorldModels">View the Project on GitHub <small>orderedlist/minimal</small></a></p>
        <ul>
          <li><a href="https://github.com/deepdad/SharedWorldModels/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/deepdad/SharedWorldModels/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="http://github.com/deepdad/SharedWorldModels">Fork On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h1>Dreamer V1</h1>

        <p><em>View the <a href="http://github.github.com/github-flavored-markdown/sample_content.html">source of this content</a>.</em></p>

        <p>The goal of this documentation is to link the paper <a href="https://arxiv.org/pdf/1912.01603.pdf" title="dreamer-v1">Dream to Control: Learning Behaviors by Latent Imagination</a> to<br>
          code snippets</p>
        
        <p>Seevral implementations exist. From the <a href="https://github.com/danijar/dreamer" title="authors">authors</a> and Google Research, another one is available in Ray's RLLib, but it is 
          currently not working. The RLLib implementation replaces the "framework code" (used to manage and run models) with ray specific instructions, but the main model code is based on an 
          implementation by Julius Frost. In our project we need a pyTorch implementation to run with RLBench. We use the Julius Frost implementation for that, while in the mean time
          targeting an RLLib contribution.</p>
        
        <h2>Back-Propagation through Random Operations</h2>
        <p>Dreamer defiunitely has a generative compartment. It accumulates on partial observation information into a distributed representation, which is
        then used to generate possible futures.</p>
        <p>To understand this aspect of Dreamer, I paraphrase Bengio et al. Traditional neural networks implement a deterministic transformation of some input 
          variables <b>x</b>. 
          When developing Dreamer's generative component, we wish to extend neural networks to implement stochastic transformations of <b>x</b>. 
          One straightforward way to do this is to augment the neural network with extra inputs <b>z</b> that are sampled from Gaussian distributions. 
          The neural network can then continue to perform deterministic computation internally, but the function f(x, z) 
          will appear stochastic to anobserver who does not have access toz. Provided that f is continuous anddifferentiable, we can then compute the gradients 
          necessary for training usingback-propagation as usual. </p>
        <p>As an example, let us consider the operation consisting of <i>drawing samples</i> y from a Gaussian 
          distribution with mean µ and variance σ2:y ∼ N(µ, σ2). (20.54) <!--Because an individual sample ofyis produced not by a function, but rather bya sampling 
          process whose output changes every time we query it, it may seemcounterintuitive to take the derivatives ofywith respect to the parameters ofits 
          distribution,µandσ2. However, we can rewrite the sampling process astransforming an underlying random valuez ∼ N(z; 0,1) to obtain a sample fromthe 
          desired distribution:y = µ + σz. (20.55)We are now able to back-propagate through the sampling operation, by regard-ing it as a deterministic operation 
          with an extra inputz. Crucially, the extra inputis a random variable whose distribution is not a function of any of the variableswhose derivatives we 
          want to calculate. The result tells us how an inﬁnitesimalchange inµorσwould change the output if we could repeat the sampling operationagain with the 
          same value of z.Being able to back-propagate through this sampling operation allows us toincorporate it into a larger graph. We can build elements of 
          the graph on top of the output of the sampling distribution. For example, we can compute the derivativesof some loss functionJ(y). We can also build 
          elements of the graph whose outputsare the inputs or the parameters of the sampling operation. For example, we couldbuild a larger graph withµ=f(x;θ) 
          andσ=g(x;θ). In this augmented graph,we can use back-propagation through these functions to derive ∇θJ(y).The principle used in this Gaussian sampling 
          example is more generally appli-cable. We can express any probability distribution of the formp(y;θ) orp(y | x;θ)asp(y | ω), whereωis a variable 
          containing both parametersθ, and if applicable,the inputsx. Given a valueysampled from distributionp(y | ω), whereωmay inturn be a function of other 
          variables, we can rewritey ∼ p(y | ω) (20.56)asy = f(z; ω), (20.57)wherezis a source of randomness. We may then compute the derivatives ofywithrespect 
          toωusing traditional tools such as the back-propagation algorithm appliedtof, as long asfis continuous and diﬀerentiable almost everywhere. Crucially, 
          ω must not be a function ofz, andzmust not be a function ofω. This techniqueis often called thereparametrization trick,stochastic back-propagation, or
          perturbation analysis.The requirement thatfbe continuous and diﬀerentiable of course requiresyto be continuous. If we wish to back-propagate through a 
          sampling process thatproduces discrete-valued samples, it may still be possible to estimate a gradient onω, using reinforcement learning algorithms such 
          as variants of the REINFORCEalgorithm (Williams, 1992), discussed in section 20.9.1.In neural network applications, we typically choosezto be drawn from 
          somesimple distribution, such as a unit uniform or unit Gaussian distribution, andachieve more complex distributions by allowing the deterministic portion 
          of thenetwork to reshape its input.The idea of propagating gradients or optimizing through stochastic operationsdates back to the mid-twentieth century 
          (Price, 1958; Bonnet, 1964) and wasﬁrst used for machine learning in the context of reinforcement learning (Williams,1992). More recently, it has been 
          applied to variational approximations (Opperand Archambeau, 2009) and stochastic and generative neural networks (Bengioet al., 2013b; Kingma, 2013; 
          Kingma and Welling, 2014b,a; Rezende et al., 2014;Goodfellow et al., 2014c). Many networks, such as denoising autoencoders ornetworks regularized with 
          dropout, are also naturally designed to take noise as an input without requiring any special reparametrization to make the noiseindependent from the model.
         <-->
         </p>
        
         <h2>RLLib</h2>
        <pre><code>
# RLLib uses trainers
# # RLlib Trainer classes coordinate the distributed workflow of running rollouts and optimizing policies. They do this by leveraging Ray parallel iterators to implement the desired computation pattern. The following figure shows synchronous sampling, the simplest of these patterns:
# RLlib uses Ray actors to scale training from a single core to many thousands of cores in a cluster. You can configure the parallelism used for training by changing the num_workers parameter. Check out our scaling guide for more details here.
DREAMERTrainer = build_trainer(
    name="Dreamer",
# look through the default config
    default_config=DEFAULT_CONFIG,
# Policies¶
# Policies are a core concept in RLlib. In a nutshell, policies are Python classes that define how an agent acts in an environment. Rollout workers query the policy to determine agent actions. In a gym environment, there is a single agent and policy. Like we saw in the practical part of the Lab.
# The Dreamer DMCEnv(core.Env) class looks more like a gym Env than a VectorEnv. Indeed, it seems to convert DM_control env to gym env., in turn a gym could be vectorized but Dreamer doesn't seem to do that. INSTEAD, EACH WORKER GETS ONE ENV.
# In vector envs, policy inference is for multiple agents at once, and in multi-agent, there may be multiple policies, each controlling one or more agents:
# Policies each define a learn_on_batch() method that improves the policy given a sample batch of input. For TF and Torch policies, this is implemented using a loss function that takes as input sample batch tensors and outputs a scalar loss
#
# Policies can be implemented using any framework. However, for TensorFlow and PyTorch, RLlib has build_tf_policy and build_torch_policy helper functions that let you define a trainable policy with a functional-style API.
# both pass the DreamerPolicy:
    default_policy=DreamerTorchPolicy,
    get_policy_class=get_policy_class,
# THE EXECUTION PLAN HERE CONTAINS DRAMERITERATION WHICH CONTAINS THE AFOREMENTIONED LEARN_ON_BATCH() METHOD:
#def execution_plan(workers, config):
#    episode_buffer = EpisodicBuffer(length=config["batch_length"])
#    local_worker = workers.local_worker()
#    rollouts = ParallelRollouts(workers)
#    rollouts = rollouts.for_each(
#        DreamerIteration(local_worker, episode_buffer, dreamer_train_iters,
#                         batch_size, act_repeat))
#            -> IN DREAMERITERATION:
#            fetches = self.worker.learn_on_batch(batch)
    execution_plan=execution_plan,
    validate_config=validate_config)
# SO, INTERESTINGLY, THIS DIFFERS FROM THE DEFAULT: LEARN_ON_BATCH EXECUTES IN A DREAMERITERATION THAT IS PART OF THE ROLLOUT OF THE PARALLELROLLOUTS(WORKERS)
# AS SUCH, THE QUESTION IS HOW THE POLICY CAN INTERACT WITH THIS NESTED CONTROL STRUCTURE? IT SEEMS POSSIBLE THAT THE POLLICY FIRST SUPPLIES THE LOCAL WORKERS THAT ARE THEN EACH GIVEN A TASK, HOWEVER THE WORKERS ARE SUPPLIED BY RAY/RLLIB/TRAINER, THE POLICY IS JUST A POLICY: A STATE->ACTION MAPPING
# ""This is the computation graph for workers (inner adaptation steps)
#  def compute_dreamer_loss""
# FROM THIS QUOTE IT SHOW THAT THE POLICY HANDS WORKERS THINGS SUCH AS A LOSS COMPUTATION
# SO IT IS THE TRAINER THAT HANDLES THE WORKERS AND THEIR JOBS AND THEY ARE GENERIC AND GIVEN A LOSS THAT FITS A TEMPLATE, THE WORKERS CAN LEARN IN THE INSTRUCTED WAY ON A SAMPLE BATCH:
# INDEED BUILD_TRAINER CONTAINS: ""
# Creating all workers (excluding evaluation workers).
#            self.workers = self._make_workers(
#                env_creator=env_creator,
#                validate_env=validate_env,
#                policy_class=self._policy_class,
#                config=config,
#                num_workers=self.config["num_workers"])
#            self.execution_plan = execution_plan
#            self.train_exec_impl = execution_plan(self.workers, config)
# ""
# _MAKE_WORKERS() THEN IS (KIND OF LAMELY) DEFINED AS
#    def _make_workers(
#            self, *, env_creator: Callable[[EnvContext], EnvType],
#            validate_env: Optional[Callable[[EnvType, EnvContext], None]],
#            policy_class: Type[Policy], config: TrainerConfigDict,
#            num_workers: int) -> WorkerSet:
#        """Default factory method for a WorkerSet running under this Trainer.
#        Override this method by passing a custom `make_workers` into
#        `build_trainer`.
#        Args:
#            env_creator (callable): A function that return and Env given an env
#                config.
#            validate_env (Optional[Callable[[EnvType, EnvContext], None]]):
#                Optional callable to validate the generated environment (only
#                on worker=0).
#            policy (Type[Policy]): The Policy class to use for creating the
#                policies of the workers.
#            config (TrainerConfigDict): The Trainer's config.
#            num_workers (int): Number of remote rollout workers to create.
#                0 for local only.
#        Returns:
#            WorkerSet: The created WorkerSet.
#        """
#        return WorkerSet(
#            env_creator=env_creator,
#            validate_env=validate_env,
#            policy_class=policy_class,
#            trainer_config=config,
#            num_workers=num_workers,
#            logdir=self.logdir)
#
# DREAMERTrainer ITSELF IS AGAIN JUST A DATA STRUCTURE, IT IS OBTAINED FROM THE COMMAND LINE AND HANDLED BY RLLIB TRAIN
</code></pre>          
          
        
       

        <p>You see, that was formatted as code because it's been indented by four spaces.</p>

        <p>How about we throw some angle braces and ampersands in there?</p>

        <pre><code>&lt;div class="footer"&gt;
    &amp;copy; 2004 Foo Corporation
&lt;/div&gt;
</code></pre>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/deepdad">Sam</a> and <a  href="https://github.com/Wetzr">Joe</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
