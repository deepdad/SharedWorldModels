<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>SharedWorldModels</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Dreamer</h1>
        <p>Shared World Models</p>
        <p class="view"><a href="http://github.com/deepdad/SharedWorldModels">View the Project on GitHub <small>orderedlist/minimal</small></a></p>
        <ul>
          <li><a href="https://github.com/deepdad/SharedWorldModels/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/deepdad/SharedWorldModels/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="http://github.com/deepdad/SharedWorldModels">Fork On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h1>Dreamer V1</h1>

        <p><em>View the <a href="http://github.github.com/github-flavored-markdown/sample_content.html">source of this content</a>.</em></p>

        <p>The goal of this documentation is to link the paper <a href="https://arxiv.org/pdf/1912.01603.pdf" title="dreamer-v1">Dream to Control: Learning Behaviors by Latent Imagination</a> to<br>
          code snippets</p>
        
        <p>Seevral implementations exist. From the <a href="https://github.com/danijar/dreamer" title="authors">authors</a> and Google Research, another one is available in Ray's RLLib, but it is 
          currently not working. The RLLib implementation replaces the "framework code" (used to manage and run models) with ray specific instructions, but the main model code is based on an 
          implementation by Julius Frost. In our project we need a pyTorch implementation to run with RLBench. We use the Julius Frost implementation for that, while in the mean time
          targeting an RLLib contribution.</p>
        
         <h2>RLLib</h2>
        <pre><code>
# RLLib uses trainers
# # RLlib Trainer classes coordinate the distributed workflow of running rollouts and optimizing policies. They do this by leveraging Ray parallel iterators to implement the desired computation pattern. The following figure shows synchronous sampling, the simplest of these patterns:
# RLlib uses Ray actors to scale training from a single core to many thousands of cores in a cluster. You can configure the parallelism used for training by changing the num_workers parameter. Check out our scaling guide for more details here.
DREAMERTrainer = build_trainer(
    name="Dreamer",
# look through the default config
    default_config=DEFAULT_CONFIG,
# PoliciesÂ¶
# Policies are a core concept in RLlib. In a nutshell, policies are Python classes that define how an agent acts in an environment. Rollout workers query the policy to determine agent actions. In a gym environment, there is a single agent and policy. Like we saw in the practical part of the Lab.
# The Dreamer DMCEnv(core.Env) class looks more like a gym Env than a VectorEnv. Indeed, it seems to convert DM_control env to gym env., in turn a gym could be vectorized but Dreamer doesn't seem to do that. INSTEAD, EACH WORKER GETS ONE ENV.
# In vector envs, policy inference is for multiple agents at once, and in multi-agent, there may be multiple policies, each controlling one or more agents:
# Policies each define a learn_on_batch() method that improves the policy given a sample batch of input. For TF and Torch policies, this is implemented using a loss function that takes as input sample batch tensors and outputs a scalar loss
#
# Policies can be implemented using any framework. However, for TensorFlow and PyTorch, RLlib has build_tf_policy and build_torch_policy helper functions that let you define a trainable policy with a functional-style API.
# both pass the DreamerPolicy:
    default_policy=DreamerTorchPolicy,
    get_policy_class=get_policy_class,
# THE EXECUTION PLAN HERE CONTAINS DRAMERITERATION WHICH CONTAINS THE AFOREMENTIONED LEARN_ON_BATCH() METHOD:
#def execution_plan(workers, config):
#    episode_buffer = EpisodicBuffer(length=config["batch_length"])
#    local_worker = workers.local_worker()
#    rollouts = ParallelRollouts(workers)
#    rollouts = rollouts.for_each(
#        DreamerIteration(local_worker, episode_buffer, dreamer_train_iters,
#                         batch_size, act_repeat))
#            -> IN DREAMERITERATION:
#            fetches = self.worker.learn_on_batch(batch)
    execution_plan=execution_plan,
    validate_config=validate_config)
# SO, INTERESTINGLY, THIS DIFFERS FROM THE DEFAULT: LEARN_ON_BATCH EXECUTES IN A DREAMERITERATION THAT IS PART OF THE ROLLOUT OF THE PARALLELROLLOUTS(WORKERS)
# AS SUCH, THE QUESTION IS HOW THE POLICY CAN INTERACT WITH THIS NESTED CONTROL STRUCTURE? IT SEEMS POSSIBLE THAT THE POLLICY FIRST SUPPLIES THE LOCAL WORKERS THAT ARE THEN EACH GIVEN A TASK, HOWEVER THE WORKERS ARE SUPPLIED BY RAY/RLLIB/TRAINER, THE POLICY IS JUST A POLICY: A STATE->ACTION MAPPING
# ""This is the computation graph for workers (inner adaptation steps)
#  def compute_dreamer_loss""
# FROM THIS QUOTE IT SHOW THAT THE POLICY HANDS WORKERS THINGS SUCH AS A LOSS COMPUTATION
# SO IT IS THE TRAINER THAT HANDLES THE WORKERS AND THEIR JOBS AND THEY ARE GENERIC AND GIVEN A LOSS THAT FITS A TEMPLATE, THE WORKERS CAN LEARN IN THE INSTRUCTED WAY ON A SAMPLE BATCH:
# INDEED BUILD_TRAINER CONTAINS: ""
# Creating all workers (excluding evaluation workers).
#            self.workers = self._make_workers(
#                env_creator=env_creator,
#                validate_env=validate_env,
#                policy_class=self._policy_class,
#                config=config,
#                num_workers=self.config["num_workers"])
#            self.execution_plan = execution_plan
#            self.train_exec_impl = execution_plan(self.workers, config)
# ""
# _MAKE_WORKERS() THEN IS (KIND OF LAMELY) DEFINED AS
#    def _make_workers(
#            self, *, env_creator: Callable[[EnvContext], EnvType],
#            validate_env: Optional[Callable[[EnvType, EnvContext], None]],
#            policy_class: Type[Policy], config: TrainerConfigDict,
#            num_workers: int) -> WorkerSet:
#        """Default factory method for a WorkerSet running under this Trainer.
#        Override this method by passing a custom `make_workers` into
#        `build_trainer`.
#        Args:
#            env_creator (callable): A function that return and Env given an env
#                config.
#            validate_env (Optional[Callable[[EnvType, EnvContext], None]]):
#                Optional callable to validate the generated environment (only
#                on worker=0).
#            policy (Type[Policy]): The Policy class to use for creating the
#                policies of the workers.
#            config (TrainerConfigDict): The Trainer's config.
#            num_workers (int): Number of remote rollout workers to create.
#                0 for local only.
#        Returns:
#            WorkerSet: The created WorkerSet.
#        """
#        return WorkerSet(
#            env_creator=env_creator,
#            validate_env=validate_env,
#            policy_class=policy_class,
#            trainer_config=config,
#            num_workers=num_workers,
#            logdir=self.logdir)
#
# DREAMERTrainer ITSELF IS AGAIN JUST A DATA STRUCTURE, IT IS OBTAINED FROM THE COMMAND LINE AND HANDLED BY RLLIB TRAIN
</code></pre>          
          
        
       

        <p>You see, that was formatted as code because it's been indented by four spaces.</p>

        <p>How about we throw some angle braces and ampersands in there?</p>

        <pre><code>&lt;div class="footer"&gt;
    &amp;copy; 2004 Foo Corporation
&lt;/div&gt;
</code></pre>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/deepdad">Sam</a> and <a  href="https://github.com/Wetzr">Joe</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
